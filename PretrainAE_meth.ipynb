{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"A7DRYwFXn2uf"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":8324,"status":"ok","timestamp":1721909367409,"user":{"displayName":"Sharmin Akter","userId":"14234100859854996291"},"user_tz":-60},"id":"sOLJCu49HsJj"},"outputs":[],"source":["import pickle\n","import numpy as np\n","import time\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.callbacks import EarlyStopping"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21667,"status":"ok","timestamp":1718383698219,"user":{"displayName":"Sharmin Akter","userId":"14234100859854996291"},"user_tz":-60},"id":"imQSYyZd6kbY","outputId":"c1cb773a-ae58-4089-895d-5ffc45faa99a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"645rJBjVpl9P"},"outputs":[],"source":["def load_data(filename):\n","    data = []\n","    gene_names = []\n","    data_labels = []\n","    lines = open(filename).readlines()\n","    sample_names = lines[0].replace('\\n', '').split('\\t')[1:]\n","    dx = 1\n","\n","    for line in lines[dx:]:\n","        values = line.replace('\\n', '').split('\\t')\n","        gene = str.upper(values[0])\n","        gene_names.append(gene)\n","        data.append(values[1:])\n","    data = np.array(data, dtype='float32')\n","    data = np.transpose(data)\n","\n","    return data, data_labels, sample_names, gene_names"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v37HhkK8pmAm"},"outputs":[],"source":["def AE_dense_3layers(input_dim, first_layer_dim, second_layer_dim, third_layer_dim, activation_func, init='he_uniform'):\n","    print('input_dim = ', input_dim)\n","    print('first_layer_dim = ', first_layer_dim)\n","    print('second_layer_dim = ', second_layer_dim)\n","    print('third_layer_dim = ', third_layer_dim)\n","    print('init = ', init)\n","\n","    model = Sequential()\n","\n","    # Encoder\n","    model.add(Dense(first_layer_dim, input_shape=(input_dim,), activation=activation_func, kernel_initializer=init))\n","    model.add(Dense(second_layer_dim, activation=activation_func, kernel_initializer=init))\n","    model.add(Dense(third_layer_dim, activation=activation_func, kernel_initializer=init))\n","\n","    # Decoder\n","    model.add(Dense(second_layer_dim, activation=activation_func, kernel_initializer=init))\n","    model.add(Dense(first_layer_dim, activation=activation_func, kernel_initializer=init))\n","    model.add(Dense(input_dim, activation=activation_func, kernel_initializer=init))\n","\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U_QG7IzmpmD9"},"outputs":[],"source":["def save_weight_to_pickle(model, file_name):\n","    print('saving weights')\n","    weight_list = []\n","    for layer in model.layers:\n","        weight_list.append(layer.get_weights())\n","    with open(file_name, 'wb') as handle:\n","        pickle.dump(weight_list, handle)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40507,"status":"ok","timestamp":1718383798988,"user":{"displayName":"Sharmin Akter","userId":"14234100859854996291"},"user_tz":-60},"id":"yRYAkzH0pmG0","outputId":"1a927261-fe23-4956-a748-3809a65de6e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Datasets successfully loaded.\n","input_dim =  4539\n","first_layer_dim =  100\n","second_layer_dim =  50\n","third_layer_dim =  25\n","init =  he_uniform\n","Epoch 1/10\n","275/275 [==============================] - 5s 4ms/step - loss: 0.0171\n","Epoch 2/10\n","275/275 [==============================] - 1s 5ms/step - loss: 0.0164\n","Epoch 3/10\n","275/275 [==============================] - 2s 6ms/step - loss: 0.0163\n","Epoch 4/10\n","275/275 [==============================] - 2s 6ms/step - loss: 0.0162\n","Epoch 5/10\n","275/275 [==============================] - 1s 4ms/step - loss: 0.0162\n","Epoch 6/10\n","275/275 [==============================] - 1s 4ms/step - loss: 0.0162\n","Epoch 7/10\n","275/275 [==============================] - 1s 4ms/step - loss: 0.0161\n","Epoch 8/10\n","275/275 [==============================] - 1s 4ms/step - loss: 0.0161\n","Epoch 9/10\n","275/275 [==============================] - 1s 4ms/step - loss: 0.0161\n","Epoch 10/10\n","275/275 [==============================] - 1s 4ms/step - loss: 0.0161\n","\n","\n","Autoencoder training completed in 0.4 mins.\n"," with testloss:0.0161\n","saving weights\n","\n","Results saved in /content/drive/MyDrive/Colab Notebooks/Thesis/results/premodel_tcga_mut_100_50_25_demo.pickle\n","\n","\n"]}],"source":["if __name__ == '__main__':\n","    # Loading data\n","    data_mut_tcga, data_labels_mut_tcga, sample_names_mut_tcga, gene_names_mut_tcga = load_data(\"/content/drive/MyDrive/Colab Notebooks/Thesis/data/tcga_meth_data_paired_with_ccl.txt\")\n","    print(\"\\n\\nDatasets successfully loaded.\")\n","\n","    # Parameters for autoencoder\n","    input_dim = data_mut_tcga.shape[1]\n","    first_layer_dim = 100\n","    second_layer_dim = 50\n","    third_layer_dim = 25\n","    batch_size = 30\n","    epoch_size = 10\n","    activation_function = 'relu'\n","    init = 'he_uniform'\n","    model_save_name = \"premodel_tcga_meth_%d_%d_%d\" % (first_layer_dim, second_layer_dim, third_layer_dim)\n","\n","    # Building autoencoder model\n","    t = time.time()\n","    model = AE_dense_3layers(input_dim=input_dim, first_layer_dim=first_layer_dim, second_layer_dim=second_layer_dim, third_layer_dim=third_layer_dim, activation_func=activation_function, init=init)\n","    model.compile(loss='mse', optimizer='adam')\n","\n","    model.fit(data_mut_tcga, data_mut_tcga, epochs=epoch_size, batch_size=batch_size, shuffle=True)\n","\n","    cost = model.evaluate(data_mut_tcga, data_mut_tcga, verbose=0)\n","    print('\\n\\nAutoencoder training completed in %.1f mins.\\n with testloss:%.4f' % ((time.time()-t)/60, cost))\n","\n","    save_weight_to_pickle(model, '/content/drive/MyDrive/Colab Notebooks/Thesis/results/' + model_save_name + '_demo.pickle')\n","    print(\"\\nResults saved in /content/drive/MyDrive/Colab Notebooks/Thesis/results/%s_demo.pickle\\n\\n\" % model_save_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g1yfC3dWpmJu"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O6_Td5_ipmMV"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"elapsed":81230,"status":"error","timestamp":1718383079412,"user":{"displayName":"Sharmin Akter","userId":"14234100859854996291"},"user_tz":-60},"id":"na4kaaqJciJL","outputId":"cd0dd26b-ab3e-4285-ce86-1ec96b2a7fde"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","Datasets successfully loaded.\n","\n","\n"]},{"ename":"ValueError","evalue":"Data cardinality is ambiguous:\n  x sizes: 1, 1, 1, 1, 1298\nMake sure all arrays contain the same number of samples.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-df3ef03434d9>\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst_to_predict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         data_pred_tmp = model_saved.predict([data_mut_tcga[z].reshape(1, -1),\n\u001b[0m\u001b[1;32m     45\u001b[0m                                              \u001b[0mdata_exp_tcga\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                                              \u001b[0mdata_cna_tcga\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1958\u001b[0m             )\n\u001b[1;32m   1959\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1960\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 1, 1, 1, 1, 1298\nMake sure all arrays contain the same number of samples."]}],"source":["def load_data(filename):\n","    data = []\n","    gene_names = []\n","    data_labels = []\n","    lines = open(filename).readlines()\n","    sample_names = lines[0].replace('\\n', '').split('\\t')[1:]\n","    dx = 1\n","\n","    for line in lines[dx:]:\n","        values = line.replace('\\n', '').split('\\t')\n","        gene = str.upper(values[0])\n","        gene_names.append(gene)\n","        data.append(values[1:])\n","    data = np.array(data, dtype='float32')\n","    data = np.transpose(data)\n","\n","    return data, data_labels, sample_names, gene_names\n","\n","def AE_dense_3layers(input_dim, first_layer_dim, second_layer_dim, third_layer_dim, activation_func, init='he_uniform'):\n","    print('input_dim = ', input_dim)\n","    print('first_layer_dim = ', first_layer_dim)\n","    print('second_layer_dim = ', second_layer_dim)\n","    print('third_layer_dim = ', third_layer_dim)\n","    print('init = ', init)\n","    model = models.Sequential()\n","    model.add(Dense(output_dim = first_layer_dim, input_dim = input_dim, activation = activation_func, init = init))\n","    model.add(Dense(output_dim = second_layer_dim, input_dim = first_layer_dim, activation = activation_func, init = init))\n","    model.add(Dense(output_dim = third_layer_dim, input_dim = second_layer_dim, activation = activation_func, init = init))\n","    model.add(Dense(output_dim = second_layer_dim, input_dim = third_layer_dim, activation = activation_func, init = init))\n","    model.add(Dense(output_dim = first_layer_dim, input_dim = second_layer_dim, activation = activation_func, init = init))\n","    model.add(Dense(output_dim = input_dim, input_dim = first_layer_dim, activation = activation_func, init = init))\n","\n","    return model\n","\n","def save_weight_to_pickle(model, file_name):\n","    print('saving weights')\n","    weight_list = []\n","    for layer in model.layers:\n","        weight_list.append(layer.get_weights())\n","    with open(file_name, 'wb') as handle:\n","        pickle.dump(weight_list, handle)\n","\n","if __name__ == '__main__':\n","    # load TCGA mutation data, substitute here with other genomics\n","    data_mut_tcga, data_labels_mut_tcga, sample_names_mut_tcga, gene_names_mut_tcga = load_data(\"/content/drive/MyDrive/Colab Notebooks/Thesis/data/tcga_mut_data_paired_with_ccl.txt\")\n","    print(\"\\n\\nDatasets successfully loaded.\")\n","\n","    samples_to_predict = np.arange(0, 50)\n","    # predict the first 50 samples for DEMO ONLY, for all samples please substitute 50 by data_mut_tcga.shape[0]\n","    # prediction results of all 8238 TCGA samples can be found in /data/premodel_tcga_*.pickle\n","\n","    input_dim = data_mut_tcga.shape[1]\n","    first_layer_dim = 100\n","    second_layer_dim = 50\n","    third_layer_dim = 25\n","    batch_size = 30\n","    epoch_size = 10\n","    activation_function = 'relu'\n","    init = 'he_uniform'\n","    model_save_name = \"premodel_tcga_mut_%d_%d_%d\" % (first_layer_dim, second_layer_dim, third_layer_dim)\n","\n","    t = time.time()\n","    model = AE_dense_3layers(input_dim = input_dim, first_layer_dim = first_layer_dim, second_layer_dim=second_layer_dim, third_layer_dim=third_layer_dim, activation_func=activation_function, init=init)\n","    model.compile(loss = 'mse', optimizer = 'adam')\n","    model.fit(data_mut_tcga[samples_to_predict], data_mut_tcga[samples_to_predict], nb_epoch=epoch_size, batch_size=batch_size, shuffle=True)\n","\n","    cost = model.evaluate(data_mut_tcga[samples_to_predict], data_mut_tcga[samples_to_predict], verbose = 0)\n","    print('\\n\\nAutoencoder training completed in %.1f mins.\\n with testloss:%.4f' % ((time.time()-t)/60, cost))\n","\n","    save_weight_to_pickle(model, '/content/drive/MyDrive/Colab Notebooks/Thesis/results/' + model_save_name + '_demo.pickle')\n","    print(\"\\nResults saved in /content/drive/MyDrive/Colab Notebooks/Thesis/results/%s_demo.pickle\\n\\n\" % model_save_name)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":96735,"status":"ok","timestamp":1718358739259,"user":{"displayName":"Sharmin Akter","userId":"08013016701752215734"},"user_tz":-60},"id":"DOZ8IAZLJTc4","outputId":"b428beeb-93d4-45ba-89bd-bbf11c03ba29"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is available\n","\n","\n","Datasets successfully loaded.\n","Epoch 1/10\n","727/727 [==============================] - 5s 6ms/step - loss: 0.0196 - val_loss: 0.0016\n","Epoch 2/10\n","727/727 [==============================] - 3s 4ms/step - loss: 9.8364e-04 - val_loss: 6.1706e-04\n","Epoch 3/10\n","727/727 [==============================] - 3s 4ms/step - loss: 4.6180e-04 - val_loss: 3.5519e-04\n","Epoch 4/10\n","727/727 [==============================] - 3s 5ms/step - loss: 3.0975e-04 - val_loss: 2.4964e-04\n","Epoch 5/10\n","727/727 [==============================] - 4s 6ms/step - loss: 2.3360e-04 - val_loss: 2.1433e-04\n","Epoch 6/10\n","727/727 [==============================] - 4s 5ms/step - loss: 2.0692e-04 - val_loss: 1.9007e-04\n","Epoch 7/10\n","727/727 [==============================] - 2s 3ms/step - loss: 1.9504e-04 - val_loss: 1.8231e-04\n","Epoch 8/10\n","727/727 [==============================] - 3s 4ms/step - loss: 1.8275e-04 - val_loss: 1.7487e-04\n","Epoch 9/10\n","727/727 [==============================] - 2s 3ms/step - loss: 1.7445e-04 - val_loss: 1.6751e-04\n","Epoch 10/10\n","727/727 [==============================] - 3s 4ms/step - loss: 1.7352e-04 - val_loss: 1.6742e-04\n","228/228 [==============================] - 1s 3ms/step - loss: 1.7737e-04\n","228/228 [==============================] - 0s 2ms/step\n","\n","\n","Autoencoder training completed in 0.7 mins.\n","loss:0.0002 valloss:0.0002 testloss:0.0002\n","Mean Squared Error (MSE): 0.0002\n","R-squared (R2): 0.6584\n","Mean Absolute Error (MAE): 0.0004\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xrjCsbidJ6kv"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8BdMFkTTJ6oY"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4MgPYlgPJUlR"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12794,"status":"ok","timestamp":1718358559040,"user":{"displayName":"Sharmin Akter","userId":"08013016701752215734"},"user_tz":-60},"id":"i0vI8xolGziM","outputId":"967fac31-f61a-490d-dc3c-95c3669785c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU is available\n","Epoch 1/10\n","200/200 [==============================] - 3s 4ms/step - loss: 0.0816 - val_loss: 0.0738\n","Epoch 2/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.0672 - val_loss: 0.0620\n","Epoch 3/10\n","200/200 [==============================] - 1s 4ms/step - loss: 0.0580 - val_loss: 0.0550\n","Epoch 4/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.0523 - val_loss: 0.0509\n","Epoch 5/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.0488 - val_loss: 0.0486\n","Epoch 6/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.0469 - val_loss: 0.0470\n","Epoch 7/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.0457 - val_loss: 0.0465\n","Epoch 8/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.0453 - val_loss: 0.0464\n","Epoch 9/10\n","200/200 [==============================] - 1s 3ms/step - loss: 0.0451 - val_loss: 0.0465\n","Epoch 10/10\n","200/200 [==============================] - 1s 4ms/step - loss: 0.0449 - val_loss: 0.0464\n","63/63 [==============================] - 0s 3ms/step - loss: 0.0465\n","63/63 [==============================] - 0s 2ms/step\n","\n","\n","Autoencoder training completed in 0.2 mins.\n","loss:0.0449 valloss:0.0464 testloss:0.0465\n","Mean Squared Error (MSE): 0.0465\n","R-squared (R2): 0.4440\n","Mean Absolute Error (MAE): 0.1746\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n","  saving_api.save_model(\n"]}],"source":["import time\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n","\n","# Function to check GPU availability\n","def check_gpu():\n","    physical_devices = tf.config.list_physical_devices('GPU')\n","    if len(physical_devices) > 0:\n","        print(\"GPU is available\")\n","    else:\n","        print(\"GPU is not available\")\n","\n","# Function to load data (dummy function for illustration purposes)\n","def load_data():\n","    # Replace with actual data loading logic\n","    data_size = 10000\n","    feature_dim = 100\n","    data_mut = np.random.rand(data_size, feature_dim)\n","    data_exp = np.random.rand(data_size, feature_dim)\n","    data_cna = np.random.rand(data_size, feature_dim)\n","    data_meth = np.random.rand(data_size, feature_dim)\n","    data_dep = np.random.rand(data_size, 1)\n","    data_fprint = np.random.rand(data_size, feature_dim)\n","    premodel_mut = np.random.rand(feature_dim, feature_dim)\n","    premodel_exp = np.random.rand(feature_dim, feature_dim)\n","    premodel_cna = np.random.rand(feature_dim, feature_dim)\n","    premodel_meth = np.random.rand(feature_dim, feature_dim)\n","    return data_mut, data_exp, data_cna, data_meth, data_dep, data_fprint, premodel_mut, premodel_exp, premodel_cna, premodel_meth\n","\n","# Function to preprocess data (dummy function for illustration purposes)\n","def preprocess_data(data_mut, num_DepOI):\n","    # Replace with actual preprocessing logic\n","    return train_test_split(np.arange(len(data_mut)), test_size=0.2, random_state=42)\n","\n","# Function to build a simplified combined model (autoencoder)\n","def build_combined_model(input_shape, activation_func, dense_layer_dim, init):\n","    input_layer = Input(shape=(input_shape,))\n","    encoded = Dense(dense_layer_dim, activation=activation_func, kernel_initializer=init)(input_layer)\n","    decoded = Dense(input_shape, activation='sigmoid')(encoded)\n","    autoencoder = Model(input_layer, decoded)\n","    autoencoder.compile(optimizer=Adam(), loss='mse')\n","    return autoencoder\n","\n","# Function to train the model\n","def train_model(model, data_mut, id_train, num_epoch, batch_size):\n","    x_train = data_mut[id_train]\n","    history = model.fit(x_train, x_train, epochs=num_epoch, batch_size=batch_size, validation_split=0.2, verbose=1)\n","    return history\n","\n","# Function to evaluate the model\n","def evaluate_model(model, data_mut, id_test, batch_size):\n","    x_test = data_mut[id_test]\n","    test_loss = model.evaluate(x_test, x_test, batch_size=batch_size)\n","    return x_test, model.predict(x_test), test_loss\n","\n","# Function to calculate additional evaluation metrics\n","def calculate_metrics(y_true, y_pred):\n","    mse = mean_squared_error(y_true, y_pred)\n","    r2 = r2_score(y_true, y_pred)\n","    mae = mean_absolute_error(y_true, y_pred)\n","    return mse, r2, mae\n","\n","# Function to save the model\n","def save_model(model, filepath):\n","    model.save(filepath)\n","\n","# Main execution\n","if __name__ == '__main__':\n","    check_gpu()\n","    data_mut, data_exp, data_cna, data_meth, data_dep, data_fprint, premodel_mut, premodel_exp, premodel_cna, premodel_meth = load_data()\n","\n","    num_DepOI = 1298\n","    activation_func = 'relu'\n","    init = 'he_uniform'\n","    dense_layer_dim = 50  # Reduced layer dimension\n","    batch_size = 32  # Reduced batch size\n","    num_epoch = 10\n","\n","    id_train, id_test = preprocess_data(data_mut, num_DepOI)\n","\n","    t = time.time()\n","    model_final = build_combined_model(data_mut.shape[1], activation_func, dense_layer_dim, init)\n","\n","    history = train_model(model_final, data_mut, id_train, num_epoch, batch_size)\n","\n","    x_test, y_pred, test_loss = evaluate_model(model_final, data_mut, id_test, batch_size)\n","    mse, r2, mae = calculate_metrics(x_test, y_pred)\n","\n","    print(\"\\n\\nAutoencoder training completed in %.1f mins.\\nloss:%.4f valloss:%.4f testloss:%.4f\" % (\n","        (time.time() - t)/60, history.history['loss'][-1],\n","        history.history['val_loss'][-1], test_loss))\n","\n","    print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n","    print(f\"R-squared (R2): {r2:.4f}\")\n","    print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n","\n","    save_model(model_final, \"model_demo.h5\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3NV2lErjJ5uV"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ogVbUF9eJ502"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WIje3HBYGzpW"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YB7jgwQIGzs_"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Te-2y-MR9pL8"},"outputs":[],"source":["import pickle\n","from keras import models\n","from keras.layers import Dense, Concatenate, Input\n","from keras.callbacks import EarlyStopping\n","import numpy as np\n","import time\n","import tensorflow as tf\n","\n","def check_gpu():\n","    physical_devices = tf.config.list_physical_devices('GPU')\n","    if len(physical_devices) > 0:\n","        print(\"GPU acceleration enabled\")\n","        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","    else:\n","        print(\"GPU acceleration NOT enabled. If using Colab, have you changed the runtime type and selected GPU as the hardware accelerator?\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RgNOtOcrGF9c"},"outputs":[],"source":["def load_data():\n","    with open('/content/drive/MyDrive/Colab Notebooks/Thesis/data/ccl_complete_data_28CCL_1298DepOI_36344samples_demo.pickle', 'rb') as f:\n","        data_mut, data_exp, data_cna, data_meth, data_dep, data_fprint = pickle.load(f)\n","\n","    premodel_mut = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Thesis/data/premodel_tcga_mut_1000_100_50.pickle', 'rb'))\n","    premodel_exp = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Thesis/data/premodel_tcga_exp_500_200_50.pickle', 'rb'))\n","    premodel_cna = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Thesis/data/premodel_tcga_cna_500_200_50.pickle', 'rb'))\n","    premodel_meth = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Thesis/data/premodel_tcga_meth_500_200_50.pickle', 'rb'))\n","    print(\"\\n\\nDatasets successfully loaded.\")\n","    return data_mut, data_exp, data_cna, data_meth, data_dep, data_fprint, premodel_mut, premodel_exp, premodel_cna, premodel_meth"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wwqnXN0sGMxL"},"outputs":[],"source":["def preprocess_data(data_mut, num_DepOI):\n","    num_ccl = int(data_mut.shape[0] / num_DepOI)\n","    id_rand = np.random.permutation(num_ccl)\n","    id_cell_train = id_rand[np.arange(0, round(num_ccl * 0.9))]\n","    id_cell_test = id_rand[np.arange(round(num_ccl * 0.9), num_ccl)]\n","\n","    id_train = np.arange(0, 1298) + id_cell_train[0] * 1298\n","    for y in id_cell_train:\n","        id_train = np.union1d(id_train, np.arange(0, 1298) + y * 1298)\n","    id_test = np.arange(0, 1298) + id_cell_test[0] * 1298\n","    for y in id_cell_test:\n","        id_test = np.union1d(id_test, np.arange(0, 1298) + y * 1298)\n","    print(\"\\n\\nTraining/validation on %d samples (%d CCLs x %d DepOIs) and testing on %d samples (%d CCLs x %d DepOIs).\\n\\n\" % (\n","        len(id_train), len(id_cell_train), num_DepOI, len(id_test), len(id_cell_test), num_DepOI))\n","    return id_train, id_test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IiIDry82GM0C"},"outputs":[],"source":["def build_submodel(input_shape, premodel, layer_units, activation_func):\n","    model = models.Sequential()\n","    for units, weights in zip(layer_units, premodel):\n","        model.add(Dense(units=units, input_shape=input_shape, activation=activation_func, weights=weights, trainable=True))\n","        input_shape = (units,)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lXUVtUGQGM3L"},"outputs":[],"source":["def build_combined_model(data_fprint, premodel_mut, premodel_exp, premodel_cna, premodel_meth, activation_func, dense_layer_dim, init):\n","    input_mut = Input(shape=(premodel_mut[0][0].shape[0],))\n","    input_exp = Input(shape=(premodel_exp[0][0].shape[0],))\n","    input_cna = Input(shape=(premodel_cna[0][0].shape[0],))\n","    input_meth = Input(shape=(premodel_meth[0][0].shape[0],))\n","    input_gene = Input(shape=(data_fprint.shape[1],))\n","\n","    model_mut = build_submodel((premodel_mut[0][0].shape[0],), premodel_mut, [1000, 100, 50], activation_func)\n","    model_exp = build_submodel((premodel_exp[0][0].shape[0],), premodel_exp, [500, 200, 50], activation_func)\n","    model_cna = build_submodel((premodel_cna[0][0].shape[0],), premodel_cna, [500, 200, 50], activation_func)\n","    model_meth = build_submodel((premodel_meth[0][0].shape[0],), premodel_meth, [500, 200, 50], activation_func)\n","\n","    model_gene = models.Sequential([\n","        Dense(units=1000, input_shape=(data_fprint.shape[1],), activation=activation_func, kernel_initializer=init, trainable=True),\n","        Dense(units=100, activation=activation_func, kernel_initializer=init, trainable=True),\n","        Dense(units=50, activation=activation_func, kernel_initializer=init, trainable=True)\n","    ])\n","\n","    combined = Concatenate()([model_mut(input_mut), model_exp(input_exp), model_cna(input_cna), model_meth(input_meth), model_gene(input_gene)])\n","    combined = Dense(units=dense_layer_dim, activation=activation_func, kernel_initializer=init)(combined)\n","    combined = Dense(units=dense_layer_dim, activation=activation_func, kernel_initializer=init)(combined)\n","    output = Dense(units=1, activation='linear', kernel_initializer=init)(combined)\n","\n","    model_final = models.Model(inputs=[input_mut, input_exp, input_cna, input_meth, input_gene], outputs=output)\n","    return model_final"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rVLZteF1GM5x"},"outputs":[],"source":["def train_model(model, data_mut, data_exp, data_cna, data_meth, data_fprint, data_dep, id_train, num_epoch, batch_size):\n","    history = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='min')\n","    model.compile(loss='mse', optimizer='adam')\n","    model.fit(\n","        [data_mut[id_train], data_exp[id_train], data_cna[id_train], data_meth[id_train], data_fprint[id_train]],\n","        data_dep[id_train], epochs=num_epoch, validation_split=1/9, batch_size=batch_size, shuffle=True,\n","        callbacks=[history])\n","    return history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2dz3GbN9GM8o"},"outputs":[],"source":["def evaluate_model(model, data_mut, data_exp, data_cna, data_meth, data_fprint, data_dep, id_test, batch_size):\n","    cost_testing = model.evaluate(\n","        [data_mut[id_test], data_exp[id_test], data_cna[id_test], data_meth[id_test], data_fprint[id_test]],\n","        data_dep[id_test], verbose=0, batch_size=batch_size)\n","    return cost_testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSeZY8sFGaiu"},"outputs":[],"source":["def save_model(model, path):\n","    model.save(path)\n","    print(\"\\n\\nFull DeepDEP model saved in %s\\n\\n\" % path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1c2o2dWhGalj","outputId":"26978a8a-e459-458e-8f66-14059ac45db9"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU acceleration enabled\n","\n","\n","Datasets successfully loaded.\n","\n","\n","Training/validation on 32450 samples (25 CCLs x 1298 DepOIs) and testing on 3894 samples (3 CCLs x 1298 DepOIs).\n","\n","\n"]}],"source":["if __name__ == '__main__':\n","    check_gpu()\n","    data_mut, data_exp, data_cna, data_meth, data_dep, data_fprint, premodel_mut, premodel_exp, premodel_cna, premodel_meth = load_data()\n","\n","    num_DepOI = 1298\n","    activation_func = 'relu'\n","    init = 'he_uniform'\n","    dense_layer_dim = 250\n","    batch_size = 50\n","    num_epoch = 10\n","\n","    id_train, id_test = preprocess_data(data_mut, num_DepOI)\n","\n","    t = time.time()\n","    model_final = build_combined_model(data_fprint, premodel_mut, premodel_exp, premodel_cna, premodel_meth, activation_func, dense_layer_dim, init)\n","\n","    history = train_model(model_final, data_mut, data_exp, data_cna, data_meth, data_fprint, data_dep, id_train, num_epoch, batch_size)\n","\n","    cost_testing = evaluate_model(model_final, data_mut, data_exp, data_cna, data_meth, data_fprint, data_dep, id_test, batch_size)\n","    print(\"\\n\\nFull DeepDEP model training completed in %.1f mins.\\nloss:%.4f valloss:%.4f testloss:%.4f\" % (\n","        (time.time() - t)/60, history.model.history['loss'][history.stopped_epoch],\n","        history.model.history['val_loss'][history.stopped_epoch], cost_testing))\n","\n","    save_model(model_final, \"/results/models/model_demo.h5\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cRWUCFbGaoq"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_VNxuhjzGari"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhkbCCzc6zTR","outputId":"53cb6a1a-0407-404a-f437-ab180e2c2493"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU acceleration enabled\n","\n","\n","Datasets successfully loaded.\n","\n","\n","Training/validation on 32450 samples (25 CCLs x 1298 DepOIs) and testing on 3894 samples (3 CCLs x 1298 DepOIs).\n","\n","\n"]}],"source":["# import pickle\n","# from keras import models\n","# from keras.layers import Dense, Concatenate, Input\n","# from keras.callbacks import EarlyStopping\n","# import numpy as np\n","# import time\n","# import tensorflow as tf\n","\n","# # Check for GPU availability\n","# physical_devices = tf.config.list_physical_devices('GPU')\n","# if len(physical_devices) > 0:\n","#     print(\"GPU acceleration enabled\")\n","#     tf.config.experimental.set_memory_growth(physical_devices[0], True)\n","# else:\n","#     print(\"GPU acceleration NOT enabled. If using Colab, have you changed the runtime type and selected GPU as the hardware accelerator?\")\n","\n","# if __name__ == '__main__':\n","#     with open('/content/drive/MyDrive/Colab Notebooks/Thesis/data/ccl_complete_data_28CCL_1298DepOI_36344samples_demo.pickle', 'rb') as f:\n","#         data_mut, data_exp, data_cna, data_meth, data_dep, data_fprint = pickle.load(f)\n","\n","#     premodel_mut = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Thesis/data/premodel_tcga_mut_1000_100_50.pickle', 'rb'))\n","#     premodel_exp = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Thesis/data/premodel_tcga_exp_500_200_50.pickle', 'rb'))\n","#     premodel_cna = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Thesis/data/premodel_tcga_cna_500_200_50.pickle', 'rb'))\n","#     premodel_meth = pickle.load(open('/content/drive/MyDrive/Colab Notebooks/Thesis/data/premodel_tcga_meth_500_200_50.pickle', 'rb'))\n","#     print(\"\\n\\nDatasets successfully loaded.\")\n","\n","#     activation_func = 'relu'\n","#     activation_func2 = 'linear'\n","#     init = 'he_uniform'\n","#     dense_layer_dim = 250\n","#     batch_size = 50\n","#     num_epoch = 10\n","#     num_DepOI = 1298\n","#     num_ccl = int(data_mut.shape[0]/num_DepOI)\n","\n","#     id_rand = np.random.permutation(num_ccl)\n","#     id_cell_train = id_rand[np.arange(0, round(num_ccl*0.9))]\n","#     id_cell_test = id_rand[np.arange(round(num_ccl*0.9), num_ccl)]\n","\n","#     id_train = np.arange(0, 1298) + id_cell_train[0]*1298\n","#     for y in id_cell_train:\n","#         id_train = np.union1d(id_train, np.arange(0, 1298) + y*1298)\n","#     id_test = np.arange(0, 1298) + id_cell_test[0] * 1298\n","#     for y in id_cell_test:\n","#         id_test = np.union1d(id_test, np.arange(0, 1298) + y*1298)\n","#     print(\"\\n\\nTraining/validation on %d samples (%d CCLs x %d DepOIs) and testing on %d samples (%d CCLs x %d DepOIs).\\n\\n\" % (\n","#         len(id_train), len(id_cell_train), num_DepOI, len(id_test), len(id_cell_test), num_DepOI))\n","\n","#     t = time.time()\n","\n","#     def build_submodel(input_shape, premodel, layer_units):\n","#         model = models.Sequential()\n","#         for units, weights in zip(layer_units, premodel):\n","#             model.add(Dense(units=units, input_shape=input_shape, activation=activation_func, weights=weights, trainable=True))\n","#             input_shape = (units,)\n","#         return model\n","\n","#     model_mut = build_submodel((premodel_mut[0][0].shape[0],), premodel_mut, [1000, 100, 50])\n","#     model_exp = build_submodel((premodel_exp[0][0].shape[0],), premodel_exp, [500, 200, 50])\n","#     model_cna = build_submodel((premodel_cna[0][0].shape[0],), premodel_cna, [500, 200, 50])\n","#     model_meth = build_submodel((premodel_meth[0][0].shape[0],), premodel_meth, [500, 200, 50])\n","\n","#     model_gene = models.Sequential([\n","#         Dense(units=1000, input_shape=(data_fprint.shape[1],), activation=activation_func, kernel_initializer=init, trainable=True),\n","#         Dense(units=100, activation=activation_func, kernel_initializer=init, trainable=True),\n","#         Dense(units=50, activation=activation_func, kernel_initializer=init, trainable=True)\n","#     ])\n","\n","#     input_mut = Input(shape=(premodel_mut[0][0].shape[0],))\n","#     input_exp = Input(shape=(premodel_exp[0][0].shape[0],))\n","#     input_cna = Input(shape=(premodel_cna[0][0].shape[0],))\n","#     input_meth = Input(shape=(premodel_meth[0][0].shape[0],))\n","#     input_gene = Input(shape=(data_fprint.shape[1],))\n","\n","#     combined = Concatenate()([model_mut(input_mut), model_exp(input_exp), model_cna(input_cna), model_meth(input_meth), model_gene(input_gene)])\n","#     combined = Dense(units=dense_layer_dim, activation=activation_func, kernel_initializer=init)(combined)\n","#     combined = Dense(units=dense_layer_dim, activation=activation_func, kernel_initializer=init)(combined)\n","#     output = Dense(units=1, activation=activation_func2, kernel_initializer=init)(combined)\n","\n","#     model_final = models.Model(inputs=[input_mut, input_exp, input_cna, input_meth, input_gene], outputs=output)\n","\n","#     history = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='min')\n","#     model_final.compile(loss='mse', optimizer='adam')\n","#     model_final.fit(\n","#         [data_mut[id_train], data_exp[id_train], data_cna[id_train], data_meth[id_train], data_fprint[id_train]],\n","#         data_dep[id_train], epochs=num_epoch, validation_split=1/9, batch_size=batch_size, shuffle=True,\n","#         callbacks=[history])\n","#     cost_testing = model_final.evaluate(\n","#         [data_mut[id_test], data_exp[id_test], data_cna[id_test], data_meth[id_test], data_fprint[id_test]],\n","#         data_dep[id_test], verbose=0, batch_size=batch_size)\n","#     print(\"\\n\\nFull DeepDEP model training completed in %.1f mins.\\nloss:%.4f valloss:%.4f testloss:%.4f\" % (\n","#         (time.time() - t)/60, history.model.history['loss'][history.stopped_epoch],\n","#         history.model.history['val_loss'][history.stopped_epoch], cost_testing))\n","#     model_final.save(\"/results/models/model_demo.h5\")\n","#     print(\"\\n\\nFull DeepDEP model saved in /results/models/model_demo.h5\\n\\n\")\n","\n","#     # Further model building for mutation-alone, expression-alone, methylation-alone, copy number-alone, and 2-omics model\n","#     # can be done similarly using the functional API and proper inputs and layers adjustments as shown above.\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
